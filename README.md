# chat-assistant

This is a chat assistant that answers questions for both DuploCloud specific or from the Web. If the question is related to DuploCloud, a RAG approach is taken to pull relevant documents from a vector store to provide context for the question. If the question is not related to DuploCloud, then assistant will answer the question with the help of web search results. The app is dockerized and can be tested locally.

## Documentation
API documentation is provided with swagger and can be viewed when running the application.

## How to Run
First start by adding 2 environment variables to a .env file at the top level of the project. Use the .env_example as a template if needed. Ensure the follow environment variables are set,

OPENAI_API_KEY  
USER_AGENT  

Next, ensure docker is running and in a terminal within the project folder run the following command,

```
bash run.sh
```

This will start the docker containers and the Swagger API interface will be available at [http://localhost:8000](http://localhost:8000) and the chat endpoint can be tested within the documentation. Stop the application either in Docker Desktop (or other relevant tool) or by sending a stop signal from the terminal (in Windows that is Ctrl+C). Restart the application by running the command above again.

## Details
When starting the application, a vector store will be generated by pulling files from a DuploCloud github repo. These files are split and embedded into a FAISS DB stored locally with a volume mount. Re-running the application will detect if the vector store is present to regenerate it or not. 

LangGraph used to create a workflow that will determine the users query context and either use a RAG approach or search the web to help answer the question. A visual of the langGraph graph is provided in the assistant subfolder.

## Future Work

- Using postgres pgvector for better persistent store options
- Using postgres for LangGraph CheckPointers to help save graph state. This could allow for partial workflow executions and human in the loop functionality.
- Giving more context to what information is available in the document store allowing more accurate decisions between RAG or a web search.
- Expanding web search to pull more info from the web instead of relying on a snippet of the search pages. 
- Expanding prompts to include other contextual information such as a user's local time, preferences, etc to better answer the questions.